# About

This module covers uploading code for the Kafka producer (a Spark notebook) that is available in Github into your Databricks workspace.  We will then mount Azure storage and run a notebook that will download the Chicago crimes dataset, and curate it in Azure storage.  We will create a table on top of it.  Post that, we will persist the Kafka bootstrap server list and the schema registry URL to a notebook for subsequent use.

