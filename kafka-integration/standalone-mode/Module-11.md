# KAFKA INTEGRATION LABS
<br>

[Return to the HDI Kafka with standalone KafkaConnect menu](README.md) | [Kafka Integration Main Menu](../README.md) <hr>

# About

This module covers creating a Databricks cluster and pinning it.  We will use Spark Structured Streaming to stream data to Kafka in a subsequent module.<br>

Navigate to your Databricks workspace.


### 1. Click on clusters
![CreateStorage01](images/04-databricks-08.png)
<br>
<hr>
<br>

### 2. Click on create clusters
![CreateStorage02](images/04-databricks-09.png)
<br>
<hr>
<br>

### 3. Enter a name, pick the number of nodes, click on start 
![CreateStorage03](images/04-databricks-10.png)
<br>
<hr>
<br>


### 4. Pin the cluster, so its metadata is persisted indefinitely; Otherwise if you dont use your workspace, and dont fire up your cluster for a few weeks, the cluster definition is deleted permanently.
![CreateStorage05](images/04-databricks-11.png)
<br>
<hr>
<br>


This concludes the module.<br>

[Return to the HDI Kafka with standalone KafkaConnect menu](README.md) | [Kafka Integration Main Menu](../README.md) <hr>
